# config/config.yaml
# Konfiguracja dla mikroserwisu LLM

llm:
  model_name: "gpt-4"    # Nazwa modelu LLM
  max_tokens: 150        # Maksymalna liczba tokenów w odpowiedzi
  temperature: 0.7       # Temperatura (kreatywność) generacji
cache:
  enabled: true          # Czy cache jest włączony
  expiration_time: 3600  # Czas ważności cache (w sekundach)



#TODO: change template